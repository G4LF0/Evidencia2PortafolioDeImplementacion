# -*- coding: utf-8 -*-
"""RegresionLogisticaHand.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F-F-ZDJqGv7aEJTtYCD92NHS6YJOgrpW

**Francisco Leonid Galvez Flores**

**A01174385**

[**Enlace al repositorio en GitHub**](https://github.com/G4LF0/Evidencia2PortafolioDeImplementacion)

# **Importacion de librerias:**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Declaramos nuestras funciones:**

Funcion de transformacion de datos:
"""

def transformacion(x_train, y_train, x_val, y_val):
  # Transformamos los registros a arreglos numericos pertenecientes a una matriz.
  # Training
  x_train = x_train.to_numpy()
  y_train = y_train.to_numpy().reshape(y_train.size, 1)
  # Validating
  x_val = x_val.to_numpy()
  y_val = y_val.to_numpy().reshape(y_val.size, 1)
  
  # Agregamos las columnas de unos a las matrices x
  # Training
  x_train = np.vstack((np.ones((x_train.shape[0], )), x_train.T)).T
  # Validating
  x_val = np.vstack((np.ones((x_val.shape[0], )), x_val.T)).T
  return x_train, y_train, x_val, y_val

"""Funcion sigmoide:"""

def sigmoid(x):
  s = 1/(1+np.exp(-(x)))
  return s

"""Funcion del algoritmo de ML:"""

def model(x_train, y_train, alpha, iterations):
  n = y_train.size # Registros → Valores de las variables

  theta = np.zeros((x_train.shape[1], 1))

  lista_costo = []
  """
  Creamos un vector con tamaño igual al numero de variables
  estos seran los pesos iniciales de nuestras variables.
  """

  for i in range(iterations):
    sigma = np.dot(x_train, theta) # Prediccion probabilistica
    y_pred = sigmoid(sigma)

    # Funcion de costo
    costo = -(1/n)*np.sum( y_train*np.log(y_pred) + (1-y_train)* np.log(1-y_pred) )

    # Gradiente descendiente
    dTheta = (1/n)*np.dot(x_train.T, y_pred-y_train)

    theta = theta - alpha*dTheta

    # Descenso de nuestra funcion de costo
    lista_costo.append(costo)

    if(i%(iterations/10) == 0): # Comentar
      print("En la iteracion numero", i, "el error es de : ", costo)

    
  return theta, lista_costo

"""Funcion de grafica costo:"""

def grafica_costo(listaCosto, n_iterations):
  rango = np.arange(0, n_iterations)
  plt.plot(rango, listaCosto)
  return plt.show()

"""Funcion para el accuracy:"""

def accuracy(x, y, theta):
  sigma = np.dot(x, theta) # Prediccion probabilistica
  y_pred = sigmoid(sigma)

  y_pred = y_pred > 0.5
  y_pred = np.array(y_pred, dtype = 'int64')
  """
  Con las dos lineas de codigo anteriores convertimos a booleanos
  y luego a un formato binario(0,1), esto recordando que con la
  funcion sigmoide los valores menores a 0.5 se tomaran como 0 
  y los mayores a 0.5 como 1
  """
  acc = (1 - np.sum(np.absolute(y_pred - y))/y.size)*100
  print("La precision del modelo es: ", round(acc, 2), "%")
  return y_pred

def model_predecir(x, theta):
  sigma = np.dot(x, theta) # Prediccion probabilistica
  y_pred = sigmoid(sigma)
  y_pred = y_pred > 0.5
  y_pred = np.array(y_pred, dtype = 'int64')
  return y_pred

"""Funcion para la matriz de confusion:

def matriz_confusion(y,y_pred):
    matrix=np.zeros((2,2)) # Formamos una matriz vacia
    for i in range(len(y)): 
        if int(y[i])==1 and int(y_pred[i])==1:  # Verdaderos positivos
            matrix[1,1]+=1
        elif int(y[i])==0 and int(y_pred[i])==1: #Falsos positivos
            matrix[0,1]+=1
        elif int(y[i])==1 and int(y_pred[i])==0: # Falsos negativos
            matrix[1,0]+=1
        elif int(y[i])==0 and int(y_pred[i])==0: #Verdaderos negativos
            matrix[0,0]+=1 
    tp = matrix[0,0] # TP 
    fp = matrix[0,1] # FP
    fn = matrix[1,0] # FN
    tn = matrix[1,1] # TN
    accuracy = (tp+tn)/(tp+fp+fn+tn)
    print("Accuracy",accuracy)

    precision= (tp)/(tp+fp) 
    print("Precision:",precision)
    
    sensibilidad = (tp)/(tp+fn)
    print("Recall:", sensibilidad)

    specificity = (tn)/(tn+fp)
    print("Specificity:",specificity)

    f1 = 2*(precision * sensibilidad)/(precision + sensibilidad)
    print("F1:", f1)

    return matrix

El codigo anterior señala como debe ser correctamente, el detalle aqui es como lo hicimos para n variables, le hicimos transformacion a los datos y estos estan afectando los resultados.
"""

def matriz_confusion(y,y_pred):
    matrix=np.zeros((2,2)) # Formamos una matriz vacia
    for i in range(len(y)): 
        if int(y[i])==1 and int(y_pred[i])==1:  # Verdaderos positivos
            matrix[1,1]+=1
        elif int(y[i])==0 and int(y_pred[i])==1: #Falsos positivos
            matrix[0,1]+=1
        elif int(y[i])==1 and int(y_pred[i])==0: # Falsos negativos
            matrix[1,0]+=1
        elif int(y[i])==0 and int(y_pred[i])==0: #Verdaderos negativos
            matrix[0,0]+=1 
    tp = matrix[1,1] # TP 
    fp = matrix[0,1] # FP
    fn = matrix[1,0] # FN
    tn = matrix[0,0] # TN
    accuracy = (tp+tn)/(tp+fp+fn+tn)
    print("Accuracy",accuracy)

    precision= (tp)/(tp+fp) 
    print("Precision:",precision)
    
    sensibilidad = (tp)/(tp+fn)
    print("Recall:", sensibilidad)

    specificity = (tn)/(tn+fp)
    print("Specificity:",specificity)

    f1 = 2*(precision * sensibilidad)/(precision + sensibilidad)
    print("F1:", f1)

    return matrix

"""Funcion para la division de datos:"""

def division(df, p_train):
  # p_train = 0.70  Porcentaje de train.

  df['is_train'] = np.random.uniform(0, 1, len(df)) <= p_train
  train, val = df[df['is_train']==True], df[df['is_train']==False]
  df = df.drop('is_train', 1)
  train = train.drop(["is_train"], axis = 1)
  val = val.drop(["is_train"], axis = 1)
  return train, val

"""# **Importamos la base de datos:**"""

url = "https://raw.githubusercontent.com/G4LF0/Evidencia2PortafolioDeImplementacion/main/breast-cancer.csv"
df = pd.read_csv(url)
df.head(2)

df = df.drop(["id"], axis = 1)
df.diagnosis = df.diagnosis.map({"M":1, "B":0})
df.shape

"""Realizamos un heatmap para ver que variables estan influyen mas, ya sea positiva o negativamente sobre nuestra variable dependiente, que en este caso sera 'diagnosis'."""

sns.set(rc = {'figure.figsize':(25,16)})
sns.heatmap(df.corr(), annot=True, cmap= 'YlGnBu')

"""Eliminamos las columnas que tienen minima relacion o relacion nula con nuestra variable a predecir."""

df = df.drop(["texture_mean",
              "smoothness_mean",
              "compactness_mean",
              "symmetry_mean",
              "fractal_dimension_mean",
              "radius_se",
              "texture_se",
              "perimeter_se",
              "area_se",
              "smoothness_se",
              "compactness_se",
              "concavity_se",
              "concave points_se",
              "symmetry_se",
              "fractal_dimension_se",
              "texture_worst",
              "smoothness_worst",
              "compactness_worst",
              "concavity_worst",
              "symmetry_worst",
              "fractal_dimension_worst"], axis = 1)
df.head(1)

"""Volvemos a realizar un heatmap:"""

sns.set(rc = {'figure.figsize':(25,16)})
sns.heatmap(df.corr(), annot=True, cmap= 'YlGnBu')

"""# **Division del dataset:**

Dividiremos el dataframe a mano con ayuda de la libreria numpy, se divide de manera aleatoria, 70% del dataframe sera para training y 30% para validating.
"""

p_train = 0.90
train, test = division(df, p_train)
p_train_2 = 0.70
train, val = division(train, p_train_2)

train.head(2)

val.head(2)

df_mcd = pd.DataFrame({"Datos: ":["Training", "Validating", "Testing"],
                       "Registros": [train.shape[0], val.shape[0], test.shape[0]]})
df_mcd

"""Declaramos nuestras variables dependientes y la variable independiente tanto en el dataframe de training como en el de testing.

Training:
"""

x_train = train.drop(["diagnosis"], axis = 1)
y_train = train[["diagnosis"]]

"""Validating:"""

x_val = val.drop(["diagnosis"], axis = 1)
y_val = val[["diagnosis"]]

"""# **Implementacion del algoritmo:**

Transformamos los datos con las ecuaciones matematicas:
"""

x_train, y_train, x_val, y_val = transformacion(x_train, y_train, x_val, y_val)

"""Algoritmo de regresion logistica:"""

iterations = 10000
alpha = 0.0000015 # Cantidad de ceros: 9
theta, lista_costo = model(x_train, y_train, alpha, iterations)

"""Graficamos la disminucion del error segun el numero de iteraciones para ver en que momento existe un estancamiento del algoritmo."""

plt.plot(np.arange(iterations), lista_costo)
plt.show()

"""# **Accuracy**

Ahora veremos la precision del modelo tanto para el dataset de training como para el de validating.

Calculamos la precision en el dataset de entrenamiento:
"""

y_pred_train = accuracy(x_train, y_train, theta)

y_pred_val = accuracy(x_val, y_val, theta)

"""# **Pruebas a mano:**"""

matriz_confusion(y_train, y_pred_train)

matriz_confusion(y_val, y_pred_val)

"""# **Pruebas con librerias:**"""

from sklearn.metrics import confusion_matrix
confusion_matrix(y_train, y_pred_train)

from sklearn.metrics import accuracy_score
accuracy_score(y_train, y_pred_train)

from sklearn.metrics import precision_score
precision_score(y_train, y_pred_train, average='binary')

from sklearn.metrics import recall_score
recall_score(y_train, y_pred_train, average='binary')

recall_score(y_train, y_pred_train, pos_label=0)

from sklearn.metrics import f1_score
f1_score(y_train, y_pred_train)

from sklearn.metrics import classification_report
print(classification_report(y_train, y_pred_train))

"""# **Predicciones:**"""

x_val.shape

l = np.random.randint(low=0, high=174)
x_pred = x_val[l:l+5]

y_pred = model_predecir(x_pred, theta)

y_pred

pd.DataFrame({"Numero de prediccion":["1", "2", "3", "4", "5"],
              "Prediccion":[y_pred[0][0], y_pred[1][0], y_pred[2][0], y_pred[3][0], y_pred[4][0]]})