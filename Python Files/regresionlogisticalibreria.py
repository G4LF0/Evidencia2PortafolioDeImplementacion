# -*- coding: utf-8 -*-
"""RegresionLogisticaLibreria.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BRxWckYBaov04MboBnAqRWaDwdpL1_4r

**Francisco Leonid Galvez Flores**

**A01174385**

[**Enlace al repositorio en GitHub**](https://github.com/G4LF0/Evidencia2PortafolioDeImplementacion)

# **Librerias:**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""Importamos las librerias necesarias para algunos procesos a realizar antes de la implementacion del modelo.

# **Importacion de la base de datos:**
"""

url = "https://raw.githubusercontent.com/G4LF0/MLAlgorithmFramework/main/breast-cancer.csv"

df = pd.read_csv(url)
df.head(2)

"""Declaramos el dataframe con ayuda de la libreria de pandas, la base de datos se encuentra alojada en nuestro repositorio en GitHub.

# **Analisis exploratorio de los datos:**
"""

df.isnull().sum()

df.diagnosis = df.diagnosis.map({"M":1, "B":0})
df.shape

sns.set(rc = {'figure.figsize':(25,16)})
sns.heatmap(df.corr(), annot=True, cmap= 'YlGnBu')

df = df.drop(["id",
              "texture_mean",
              "smoothness_mean",
              "compactness_mean",
              "symmetry_mean",
              "fractal_dimension_mean",
              "radius_se",
              "texture_se",
              "perimeter_se",
              "area_se",
              "smoothness_se",
              "compactness_se",
              "concavity_se",
              "concave points_se",
              "symmetry_se",
              "fractal_dimension_se",
              "texture_worst",
              "smoothness_worst",
              "compactness_worst",
              "concavity_worst",
              "symmetry_worst",
              "fractal_dimension_worst"], axis = 1)
df.head(1)

sns.set(rc = {'figure.figsize':(25,16)})
sns.heatmap(df.corr(), annot=True, cmap= 'YlGnBu')

"""En esta parte lo que hicimos fue analizar columnas que no tuvieran relacion con la variable a predecir, asi como tambien cambiamos el valor de algunos variables para convertirlas en binarias.

# **Division de los datos en trainig, validating and testing:**
"""

x = df.drop(["diagnosis"], axis = 1)
y = df.diagnosis

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x.values, y.values, train_size=0.9, random_state=16)

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, train_size=0.7, random_state=16)

datos_n = [x_train.shape[0], x_val.shape[0], x_test.shape[0]]

df_mcd = pd.DataFrame({"Datos: ":["Training", "Validating", "Testing"],
                       "Registros": datos_n})
df_mcd

"""En esta parte lo que hacemos es divir el data set, al principio lo dividimos en un 90% para training y un 10% para test, despues lo que hacemos es que del 30% del 90% de training lo convertimos en validating.

# **Implementacion del modelo:**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

logreg = LogisticRegression(max_iter= 10000)
logreg.fit(x_train, y_train)
y_pred = logreg.predict(x_train)

"""Implementamos el algoritmo de regresion logistica con un numero de iteraciones maxima de 10,000.

Accuracy for training:
"""

score =accuracy_score(y_train,y_pred)
accuracy_training = score
print("Accuracy for the training set is: ", round(score*100, 4))

"""Accuracy for validating:"""

y_pred = logreg.predict(x_val)
score =accuracy_score(y_val,y_pred)
accuracy_validating = score
print("Accuracy for the training set is: ", round(score*100, 4))

"""Accuracy for testing"""

y_pred = logreg.predict(x_test)
score =accuracy_score(y_test,y_pred)
accuracy_testing = score
print("Accuracy for the training set is: ", round(score*100, 4))

"""# **Metricas:**

Metricas del training:
"""

from sklearn import metrics

y_pred = logreg.predict(x_train)
confusion_matrix = metrics.confusion_matrix(y_train, y_pred)
confusion_matrix

y_train.shape

y_pred.shape

sns.heatmap(confusion_matrix, annot=True)

import warnings
warnings.filterwarnings('always')  # "error", "ignore", "always", "default", "module" or "once"

from sklearn.metrics import classification_report

target_names = ['Sin tumor', 'Con tumor']
print(classification_report(y_train, y_pred, target_names=target_names))

"""Metricas del validating:"""

from sklearn import metrics

y_pred = logreg.predict(x_val)
confusion_matrix = metrics.confusion_matrix(y_val, y_pred)
confusion_matrix

sns.heatmap(confusion_matrix, annot=True)

from sklearn.metrics import classification_report

target_names = ['Sin tumor', 'Con tumor']
print(classification_report(y_val, y_pred, target_names=target_names))

"""Metricas del testing:"""

from sklearn import metrics

y_pred = logreg.predict(x_test)
confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
confusion_matrix

sns.heatmap(confusion_matrix, annot=True)

from sklearn.metrics import classification_report

target_names = ['Sin tumor', 'Con tumor']
print(classification_report(y_test, y_pred, target_names=target_names))

"""# **Curva ROC**

Training:
"""

y_pred_proba = logreg.predict_proba(x_train)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_train,  y_pred_proba)
auc = metrics.roc_auc_score(y_train, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

"""Validating:"""

y_pred_proba = logreg.predict_proba(x_val)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_val,  y_pred_proba)
auc = metrics.roc_auc_score(y_val, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

"""Testing:"""

y_pred_proba = logreg.predict_proba(x_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

"""Tenemos que decir que la curva ROC es un grafico en el que se ponen los valores positivos y falsos en cada eje, y como podemos ver dieron un muy buen accuracy en los tres datasets.

# **Tabla de accuracys:**
"""

df_accuracys = pd.DataFrame({"Datos: ":["Training", "Validating", "Testing"],
                             "Registros": [accuracy_training, accuracy_validating, accuracy_testing]})
df_accuracys

"""Como podemos ver nuestro modelo dispone de muy poco error en el dataset de training y mucho menos error en el dataset de validacion, por lo que decimos que se encuentra en **balance**.

# **Predicciones:**
"""

df.head(2)

"""
from sklearn.datasets import make_blobs
x_pred, _ = make_blobs(n_samples=5, centers=2, n_features=25, random_state=1)
y_pred = logreg.predict(x_pred)
for i in range(len(x_pred)):
  print("X=%s, Prediccion=%s" % (x_pred[i], y_pred[i]))
"""

l = np.random.randint(low=0, high=52)
 x_pred = x_test[l:l+5]

y_pred = logreg.predict(x_pred)

y_pred

pd.DataFrame({"Numero de prediccion":["1", "2", "3", "4", "5"],
              "Prediccion":y_pred})